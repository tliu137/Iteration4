{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BDAS_Datass transformation').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "               StructField('year',IntegerType(),True),\n",
    "               StructField('month',IntegerType(),True),\n",
    "               StructField('day',IntegerType(),True),\n",
    "               StructField('hour',IntegerType(),True),\n",
    "               StructField('PM25',FloatType(),True),\n",
    "               StructField('SO2',FloatType(),True),\n",
    "               StructField('NO2',FloatType(),True),\n",
    "               StructField('CO',FloatType(),True),\n",
    "               StructField('O3',FloatType(),True),\n",
    "               StructField('TEMP',FloatType(),True),\n",
    "               StructField('PRES',FloatType(),True),\n",
    "               StructField('DewPointTempeature',FloatType(),True),\n",
    "               StructField('RAIN',FloatType(),True),\n",
    "               StructField('WindDirection',StringType(),True),\n",
    "               StructField('WindSpend',FloatType(),True),\n",
    "               StructField('station',StringType(),True),\n",
    "               StructField('NQR',StringType(),True),\n",
    "               StructField('Season',StringType(),True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"../final_dataset/finishedpreprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+-----+------+----+----+------+------------------+----+-------------+---------+-------------+------------+------+\n",
      "|year|month|hour| SO2|  NO2|    CO|  O3|TEMP|  PRES|DewPointTempeature|RAIN|WindDirection|WindSpend|      station|         NQR|Season|\n",
      "+----+-----+----+----+-----+------+----+----+------+------------------+----+-------------+---------+-------------+------------+------+\n",
      "|2015|    1|   0|10.0| 16.0| 400.0|54.0|-1.0|1027.0|             -23.0| 0.0|           NW|      0.9|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   1|11.0| 17.0| 400.0|53.0| 0.0|1025.0|             -22.9| 0.0|           NW|      2.7|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   2|10.0| 15.0| 400.0|55.0| 0.0|1027.0|             -22.9| 0.0|           NW|      2.4|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   3|13.0| 13.0| 400.0|57.0| 0.0|1028.0|             -24.4| 0.0|           NW|      2.4|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   4|15.0| 12.0| 400.0|58.0| 0.0|1030.0|             -24.4| 0.0|           NW|      2.4|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   5|13.0| 15.0| 500.0|55.0|-1.0|1024.0|             -24.4| 0.0|          WNW|      3.2|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   6|10.0| 30.0| 500.0|39.0|-4.0|1029.0|             -24.3| 0.0|            W|      0.9|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   7|10.0| 41.0| 600.0|30.0|-2.0|1029.0|             -23.9| 0.0|          WNW|      4.2|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   8|10.0| 55.0| 600.0|16.0|-2.0|1027.0|             -23.9| 0.0|          WNW|      1.9|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|   9|11.0| 56.0| 600.0|17.0|-1.0|1026.0|             -23.0| 0.0|          SSW|      1.5|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  10|15.0| 56.0| 900.0|21.0| 0.0|1025.0|             -22.9| 0.0|           SW|      2.1|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  11|26.0| 64.0|1300.0|23.0| 1.0|1029.0|             -22.8| 0.0|           SW|      1.4|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  12|42.0| 76.0|1700.0|27.0| 1.0|1025.0|             -22.8| 0.0|           SW|      2.5|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  13|37.0| 70.0|1500.0|37.0| 2.0|1025.0|             -22.8| 0.0|          SSW|      1.8|Wanshouxigong|Satisfactory|Winter|\n",
      "|2015|    1|  14|29.0| 59.0|1300.0|49.0| 3.0|1022.0|             -22.8| 0.0|           SW|      2.0|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  15|18.0| 33.0| 900.0|64.0| 3.0|1020.0|             -22.8| 0.0|           SW|      2.4|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  16|19.0| 39.0|1000.0|60.0| 3.0|1020.0|             -22.0| 0.0|           SW|      2.7|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  17|23.0| 58.0|1300.0|46.0| 1.0|1021.0|             -22.1| 0.0|           SW|      1.1|Wanshouxigong|        Good|Winter|\n",
      "|2015|    1|  18|35.0| 89.0|1800.0|25.0|-1.0|1017.0|             -20.1| 0.0|          SSW|      1.2|Wanshouxigong|Satisfactory|Winter|\n",
      "|2015|    1|  19|43.0|106.0|2000.0|11.0|-2.0|1021.0|             -19.9| 0.0|            W|      1.1|Wanshouxigong|Satisfactory|Winter|\n",
      "+----+-----+----+----+-----+------+----+----+------+------------------+----+-------------+---------+-------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(*[\"PM25\",\"day\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "column = ['WindDirection','station','NQR','Season']\n",
    "for i in column:\n",
    "    indexer = StringIndexer(inputCol =i, outputCol= i+\"Index\" )\n",
    "    model = indexer.fit(df)\n",
    "    indexed = model.transform(df)\n",
    "    df = indexed\n",
    "new_df = df.drop(*column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "formula = RFormula(\n",
    "    formula=\"NQRIndex ~ .\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(new_df).transform(new_df)\n",
    "vectorformat = output.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorformat.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 16 features selected\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=16, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "result = selector.fit(vectorformat).transform(vectorformat)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                   |label|selectedFeatures                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------+\n",
      "|[2015.0,1.0,0.0,10.0,16.0,400.0,54.0,-1.0,1027.0,-23.0,0.0,0.8999999761581421,1.0,4.0,0.0]                 |0.0  |[2015.0,1.0,0.0,10.0,16.0,400.0,54.0,-1.0,1027.0,-23.0,0.0,0.8999999761581421,1.0,4.0,0.0]                 |\n",
      "|[2015.0,1.0,1.0,11.0,17.0,400.0,53.0,0.0,1025.0,-22.899999618530273,0.0,2.700000047683716,1.0,4.0,0.0]     |0.0  |[2015.0,1.0,1.0,11.0,17.0,400.0,53.0,0.0,1025.0,-22.899999618530273,0.0,2.700000047683716,1.0,4.0,0.0]     |\n",
      "|[2015.0,1.0,2.0,10.0,15.0,400.0,55.0,0.0,1027.0,-22.899999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |0.0  |[2015.0,1.0,2.0,10.0,15.0,400.0,55.0,0.0,1027.0,-22.899999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,3.0,13.0,13.0,400.0,57.0,0.0,1028.0,-24.399999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |0.0  |[2015.0,1.0,3.0,13.0,13.0,400.0,57.0,0.0,1028.0,-24.399999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,4.0,15.0,12.0,400.0,58.0,0.0,1030.0,-24.399999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |0.0  |[2015.0,1.0,4.0,15.0,12.0,400.0,58.0,0.0,1030.0,-24.399999618530273,0.0,2.4000000953674316,1.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,5.0,13.0,15.0,500.0,55.0,-1.0,1024.0,-24.399999618530273,0.0,3.200000047683716,9.0,4.0,0.0]    |0.0  |[2015.0,1.0,5.0,13.0,15.0,500.0,55.0,-1.0,1024.0,-24.399999618530273,0.0,3.200000047683716,9.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,6.0,10.0,30.0,500.0,39.0,-4.0,1029.0,-24.299999237060547,0.0,0.8999999761581421,15.0,4.0,0.0]  |0.0  |[2015.0,1.0,6.0,10.0,30.0,500.0,39.0,-4.0,1029.0,-24.299999237060547,0.0,0.8999999761581421,15.0,4.0,0.0]  |\n",
      "|[2015.0,1.0,7.0,10.0,41.0,600.0,30.0,-2.0,1029.0,-23.899999618530273,0.0,4.199999809265137,9.0,4.0,0.0]    |0.0  |[2015.0,1.0,7.0,10.0,41.0,600.0,30.0,-2.0,1029.0,-23.899999618530273,0.0,4.199999809265137,9.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,8.0,10.0,55.0,600.0,16.0,-2.0,1027.0,-23.899999618530273,0.0,1.899999976158142,9.0,4.0,0.0]    |0.0  |[2015.0,1.0,8.0,10.0,55.0,600.0,16.0,-2.0,1027.0,-23.899999618530273,0.0,1.899999976158142,9.0,4.0,0.0]    |\n",
      "|[2015.0,1.0,9.0,11.0,56.0,600.0,17.0,-1.0,1026.0,-23.0,0.0,1.5,10.0,4.0,0.0]                               |0.0  |[2015.0,1.0,9.0,11.0,56.0,600.0,17.0,-1.0,1026.0,-23.0,0.0,1.5,10.0,4.0,0.0]                               |\n",
      "|[2015.0,1.0,10.0,15.0,56.0,900.0,21.0,0.0,1025.0,-22.899999618530273,0.0,2.0999999046325684,4.0,4.0,0.0]   |0.0  |[2015.0,1.0,10.0,15.0,56.0,900.0,21.0,0.0,1025.0,-22.899999618530273,0.0,2.0999999046325684,4.0,4.0,0.0]   |\n",
      "|[2015.0,1.0,11.0,26.0,64.0,1300.0,23.0,1.0,1029.0,-22.799999237060547,0.0,1.399999976158142,4.0,4.0,0.0]   |0.0  |[2015.0,1.0,11.0,26.0,64.0,1300.0,23.0,1.0,1029.0,-22.799999237060547,0.0,1.399999976158142,4.0,4.0,0.0]   |\n",
      "|[2015.0,1.0,12.0,42.0,76.0,1700.0,27.0,1.0,1025.0,-22.799999237060547,0.0,2.5,4.0,4.0,0.0]                 |0.0  |[2015.0,1.0,12.0,42.0,76.0,1700.0,27.0,1.0,1025.0,-22.799999237060547,0.0,2.5,4.0,4.0,0.0]                 |\n",
      "|[2015.0,1.0,13.0,37.0,70.0,1500.0,37.0,2.0,1025.0,-22.799999237060547,0.0,1.7999999523162842,10.0,4.0,0.0] |1.0  |[2015.0,1.0,13.0,37.0,70.0,1500.0,37.0,2.0,1025.0,-22.799999237060547,0.0,1.7999999523162842,10.0,4.0,0.0] |\n",
      "|[2015.0,1.0,14.0,29.0,59.0,1300.0,49.0,3.0,1022.0,-22.799999237060547,0.0,2.0,4.0,4.0,0.0]                 |0.0  |[2015.0,1.0,14.0,29.0,59.0,1300.0,49.0,3.0,1022.0,-22.799999237060547,0.0,2.0,4.0,4.0,0.0]                 |\n",
      "|[2015.0,1.0,15.0,18.0,33.0,900.0,64.0,3.0,1020.0,-22.799999237060547,0.0,2.4000000953674316,4.0,4.0,0.0]   |0.0  |[2015.0,1.0,15.0,18.0,33.0,900.0,64.0,3.0,1020.0,-22.799999237060547,0.0,2.4000000953674316,4.0,4.0,0.0]   |\n",
      "|[2015.0,1.0,16.0,19.0,39.0,1000.0,60.0,3.0,1020.0,-22.0,0.0,2.700000047683716,4.0,4.0,0.0]                 |0.0  |[2015.0,1.0,16.0,19.0,39.0,1000.0,60.0,3.0,1020.0,-22.0,0.0,2.700000047683716,4.0,4.0,0.0]                 |\n",
      "|[2015.0,1.0,17.0,23.0,58.0,1300.0,46.0,1.0,1021.0,-22.100000381469727,0.0,1.100000023841858,4.0,4.0,0.0]   |0.0  |[2015.0,1.0,17.0,23.0,58.0,1300.0,46.0,1.0,1021.0,-22.100000381469727,0.0,1.100000023841858,4.0,4.0,0.0]   |\n",
      "|[2015.0,1.0,18.0,35.0,89.0,1800.0,25.0,-1.0,1017.0,-20.100000381469727,0.0,1.2000000476837158,10.0,4.0,0.0]|1.0  |[2015.0,1.0,18.0,35.0,89.0,1800.0,25.0,-1.0,1017.0,-20.100000381469727,0.0,1.2000000476837158,10.0,4.0,0.0]|\n",
      "|[2015.0,1.0,19.0,43.0,106.0,2000.0,11.0,-2.0,1021.0,-19.899999618530273,0.0,1.100000023841858,15.0,4.0,0.0]|1.0  |[2015.0,1.0,19.0,43.0,106.0,2000.0,11.0,-2.0,1021.0,-19.899999618530273,0.0,1.100000023841858,15.0,4.0,0.0]|\n",
      "+-----------------------------------------------------------------------------------------------------------+-----+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('features','label','selectedFeatures').show(20,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=int(9), maxDepth=int(5), labelCol='label', seed=11)\n",
    "model = rf.fit(vectorformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NQRIndex': 0.0, 'year': 0.00013154716809603672, 'DewPointTempeature': 0.0884079368264622, 'stationIndex': 0.001036175290576831, 'NO2': 0.1810933064930174, 'CO': 0.39062896097298794, 'WindDirectionIndex': 0.02110121663588012, 'month': 0.009670148938959573, 'TEMP': 0.024795955307260142, 'hour': 0.0005757787124201702, 'PRES': 0.02374150268508552, 'O3': 0.046052694563309626, 'RAIN': 0.0, 'WindSpend': 0.002673793209110729, 'SO2': 0.2100909831968337}\n"
     ]
    }
   ],
   "source": [
    "ff=model.featureImportances\n",
    "importancesList=[float(col) for col in  ff]\n",
    "colList = new_df.columns\n",
    "result=dict(zip(colList,importancesList))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(d): \n",
    "    items=d.items() \n",
    "    backitems=[[v[1],v[0]] for v in items] \n",
    "    backitems.sort() \n",
    "    return [ backitems[i][1] for i in range(0,len(backitems))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = sort_by_value(result)\n",
    "dic.reverse()\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "|month| SO2|  NO2|    CO|  O3|TEMP|  PRES|DewPointTempeature|WindSpend|WindDirectionIndex|stationIndex|NQRIndex|SeasonIndex|\n",
      "+-----+----+-----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "|    1|10.0| 16.0| 400.0|54.0|-1.0|1027.0|             -23.0|      0.9|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|11.0| 17.0| 400.0|53.0| 0.0|1025.0|             -22.9|      2.7|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0| 15.0| 400.0|55.0| 0.0|1027.0|             -22.9|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|13.0| 13.0| 400.0|57.0| 0.0|1028.0|             -24.4|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|15.0| 12.0| 400.0|58.0| 0.0|1030.0|             -24.4|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|13.0| 15.0| 500.0|55.0|-1.0|1024.0|             -24.4|      3.2|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0| 30.0| 500.0|39.0|-4.0|1029.0|             -24.3|      0.9|              15.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0| 41.0| 600.0|30.0|-2.0|1029.0|             -23.9|      4.2|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0| 55.0| 600.0|16.0|-2.0|1027.0|             -23.9|      1.9|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|11.0| 56.0| 600.0|17.0|-1.0|1026.0|             -23.0|      1.5|              10.0|         4.0|     0.0|        0.0|\n",
      "|    1|15.0| 56.0| 900.0|21.0| 0.0|1025.0|             -22.9|      2.1|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|26.0| 64.0|1300.0|23.0| 1.0|1029.0|             -22.8|      1.4|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|42.0| 76.0|1700.0|27.0| 1.0|1025.0|             -22.8|      2.5|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|37.0| 70.0|1500.0|37.0| 2.0|1025.0|             -22.8|      1.8|              10.0|         4.0|     1.0|        0.0|\n",
      "|    1|29.0| 59.0|1300.0|49.0| 3.0|1022.0|             -22.8|      2.0|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|18.0| 33.0| 900.0|64.0| 3.0|1020.0|             -22.8|      2.4|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|19.0| 39.0|1000.0|60.0| 3.0|1020.0|             -22.0|      2.7|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|23.0| 58.0|1300.0|46.0| 1.0|1021.0|             -22.1|      1.1|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|35.0| 89.0|1800.0|25.0|-1.0|1017.0|             -20.1|      1.2|              10.0|         4.0|     1.0|        0.0|\n",
      "|    1|43.0|106.0|2000.0|11.0|-2.0|1021.0|             -19.9|      1.1|              15.0|         4.0|     1.0|        0.0|\n",
      "+-----+----+-----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of attribute:  13\n"
     ]
    }
   ],
   "source": [
    "after_feature = new_df.drop(*['hour','RAIN','year'])\n",
    "after_feature.show()\n",
    "print(\"Number of attribute: \",len(after_feature.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = RFormula(\n",
    "    formula=\"NQRIndex ~ .\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(after_feature).transform(after_feature)\n",
    "vectorformat2 = output.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108279 49511 38691 12271 3658 1791\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "df_0 = after_feature.filter(col(\"NQRIndex\") == 0)\n",
    "df_1 = after_feature.filter(col(\"NQRIndex\") == 1)\n",
    "df_2 = after_feature.filter(col(\"NQRIndex\") == 2)\n",
    "df_3 = after_feature.filter(col(\"NQRIndex\") == 3)\n",
    "df_4 = after_feature.filter(col(\"NQRIndex\") == 4)\n",
    "df_5 = after_feature.filter(col(\"NQRIndex\") == 5)\n",
    "\n",
    "print(df_0.count(),df_1.count(),df_2.count(),df_3.count(),df_4.count(),df_5.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = [df_1,df_2,df_3,df_4,df_5]\n",
    "combined_df = df_0\n",
    "for element in count_list:\n",
    "    ratio = int(df_0.count()/element.count())\n",
    "    a = range(ratio)\n",
    "    oversampled_df = element.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "    combined_df = combined_df.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108279 99022 77382 98168 106082 107460\n"
     ]
    }
   ],
   "source": [
    "df_0 = combined_df.filter(col(\"NQRIndex\") == 0)\n",
    "df_1 = combined_df.filter(col(\"NQRIndex\") == 1)\n",
    "df_2 = combined_df.filter(col(\"NQRIndex\") == 2)\n",
    "df_3 = combined_df.filter(col(\"NQRIndex\") == 3)\n",
    "df_4 = combined_df.filter(col(\"NQRIndex\") == 4)\n",
    "df_5 = combined_df.filter(col(\"NQRIndex\") == 5)\n",
    "\n",
    "print(df_0.count(),df_1.count(),df_2.count(),df_3.count(),df_4.count(),df_5.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = combined_df\n",
    "file.write.csv(\"../final_dataset/view2.csv\",header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596393\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- O3: float (nullable = true)\n",
      " |-- TEMP: float (nullable = true)\n",
      " |-- PRES: float (nullable = true)\n",
      " |-- DewPointTempeature: float (nullable = true)\n",
      " |-- WindSpend: float (nullable = true)\n",
      " |-- WindDirectionIndex: double (nullable = true)\n",
      " |-- stationIndex: double (nullable = true)\n",
      " |-- NQRIndex: double (nullable = true)\n",
      " |-- SeasonIndex: double (nullable = true)\n",
      "\n",
      "+-----+----+----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "|month| SO2| NO2|    CO|  O3|TEMP|  PRES|DewPointTempeature|WindSpend|WindDirectionIndex|stationIndex|NQRIndex|SeasonIndex|\n",
      "+-----+----+----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "|    1|10.0|16.0| 400.0|54.0|-1.0|1027.0|             -23.0|      0.9|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|11.0|17.0| 400.0|53.0| 0.0|1025.0|             -22.9|      2.7|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0|15.0| 400.0|55.0| 0.0|1027.0|             -22.9|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|13.0|13.0| 400.0|57.0| 0.0|1028.0|             -24.4|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|15.0|12.0| 400.0|58.0| 0.0|1030.0|             -24.4|      2.4|               1.0|         4.0|     0.0|        0.0|\n",
      "|    1|13.0|15.0| 500.0|55.0|-1.0|1024.0|             -24.4|      3.2|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0|30.0| 500.0|39.0|-4.0|1029.0|             -24.3|      0.9|              15.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0|41.0| 600.0|30.0|-2.0|1029.0|             -23.9|      4.2|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|10.0|55.0| 600.0|16.0|-2.0|1027.0|             -23.9|      1.9|               9.0|         4.0|     0.0|        0.0|\n",
      "|    1|11.0|56.0| 600.0|17.0|-1.0|1026.0|             -23.0|      1.5|              10.0|         4.0|     0.0|        0.0|\n",
      "|    1|15.0|56.0| 900.0|21.0| 0.0|1025.0|             -22.9|      2.1|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|26.0|64.0|1300.0|23.0| 1.0|1029.0|             -22.8|      1.4|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|42.0|76.0|1700.0|27.0| 1.0|1025.0|             -22.8|      2.5|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|29.0|59.0|1300.0|49.0| 3.0|1022.0|             -22.8|      2.0|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|18.0|33.0| 900.0|64.0| 3.0|1020.0|             -22.8|      2.4|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|19.0|39.0|1000.0|60.0| 3.0|1020.0|             -22.0|      2.7|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|23.0|58.0|1300.0|46.0| 1.0|1021.0|             -22.1|      1.1|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|28.0|91.0|1500.0| 4.0|-1.0|1024.0|             -18.1|      1.8|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|23.0|86.0|1500.0| 3.0|-4.0|1021.0|             -16.6|      0.7|               4.0|         4.0|     0.0|        0.0|\n",
      "|    1|17.0|62.0|1000.0|16.0|-1.0|1020.0|             -18.1|      3.5|              14.0|         4.0|     0.0|        0.0|\n",
      "+-----+----+----+------+----+----+------+------------------+---------+------------------+------------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.printSchema()\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596393"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = RFormula(\n",
    "    formula=\"NQRIndex ~ .\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(combined_df).transform(combined_df)\n",
    "vectorformat3 = output.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,10.0,16.0,40...|  0.0|\n",
      "|[1.0,11.0,17.0,40...|  0.0|\n",
      "|[1.0,10.0,15.0,40...|  0.0|\n",
      "|[1.0,13.0,13.0,40...|  0.0|\n",
      "|[1.0,15.0,12.0,40...|  0.0|\n",
      "|[1.0,13.0,15.0,50...|  0.0|\n",
      "|[1.0,10.0,30.0,50...|  0.0|\n",
      "|[1.0,10.0,41.0,60...|  0.0|\n",
      "|[1.0,10.0,55.0,60...|  0.0|\n",
      "|[1.0,11.0,56.0,60...|  0.0|\n",
      "|[1.0,15.0,56.0,90...|  0.0|\n",
      "|[1.0,26.0,64.0,13...|  0.0|\n",
      "|[1.0,42.0,76.0,17...|  0.0|\n",
      "|[1.0,29.0,59.0,13...|  0.0|\n",
      "|[1.0,18.0,33.0,90...|  0.0|\n",
      "|[1.0,19.0,39.0,10...|  0.0|\n",
      "|[1.0,23.0,58.0,13...|  0.0|\n",
      "|[1.0,28.0,91.0,15...|  0.0|\n",
      "|[1.0,23.0,86.0,15...|  0.0|\n",
      "|[1.0,17.0,62.0,10...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "596393"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorformat3.show()\n",
    "vectorformat3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = vectorformat3.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417708 178685\n"
     ]
    }
   ],
   "source": [
    "print(trainingData.count(),testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier, DecisionTreeClassifier)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[1.0,1.0,9.0,200....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,9.0,300....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,11.0,300...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,12.0,200...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,14.0,400...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,17.0,600...|  0.0|[6180.0,1293.0,78...|[0.81789306511381...|       0.0|\n",
      "|[1.0,1.0,25.0,300...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,34.0,100...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,1.0,60.0,100...|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "|[1.0,2.0,2.0,200....|  0.0|[23595.0,783.0,11...|[0.96125641652407...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RunTime 30.745683193206787 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dt = DecisionTreeClassifier()\n",
    "model_dt = dt.fit(trainingData)\n",
    "prediction_dt = model_dt.transform(testData)\n",
    "prediction_dt.show()\n",
    "print(\"RunTime %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|[1.0,1.0,9.0,200....|\n",
      "|       0.0|  0.0|[1.0,1.0,9.0,300....|\n",
      "|       0.0|  0.0|[1.0,1.0,11.0,300...|\n",
      "|       0.0|  0.0|[1.0,1.0,12.0,200...|\n",
      "|       0.0|  0.0|[1.0,1.0,14.0,400...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_dt.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaccuracy =  60.42253127011221\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction_dt)\n",
    "print(\"Aaccuracy = \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[1.0,1.0,9.0,200....|  0.0|[4.60572978113011...|[0.92114595622602...|       0.0|\n",
      "|[1.0,1.0,9.0,300....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,1.0,11.0,300...|  0.0|[4.60572978113011...|[0.92114595622602...|       0.0|\n",
      "|[1.0,1.0,12.0,200...|  0.0|[4.60572978113011...|[0.92114595622602...|       0.0|\n",
      "|[1.0,1.0,14.0,400...|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,1.0,17.0,600...|  0.0|[4.40919061875809...|[0.88183812375161...|       0.0|\n",
      "|[1.0,1.0,25.0,300...|  0.0|[4.66686858095273...|[0.93337371619054...|       0.0|\n",
      "|[1.0,1.0,34.0,100...|  0.0|[4.59439108060429...|[0.91887821612085...|       0.0|\n",
      "|[1.0,1.0,60.0,100...|  0.0|[3.53272635697201...|[0.70654527139440...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.60100723431752...|[0.92020144686350...|       0.0|\n",
      "|[1.0,2.0,2.0,100....|  0.0|[4.60100723431752...|[0.92020144686350...|       0.0|\n",
      "|[1.0,2.0,2.0,200....|  0.0|[4.70904629588762...|[0.94180925917752...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RunTime 34.22742986679077 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5)\n",
    "model_rf = rf.fit(trainingData)\n",
    "prediction_rf = model_rf.transform(testData)\n",
    "prediction_rf.show()\n",
    "print(\"RunTime %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaccuracy =  59.93284271203515\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(prediction_rf)\n",
    "print(\"Aaccuracy = \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o811.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 189.0 failed 1 times, most recent failure: Lost task 2.0 in stage 189.0 (TID 637, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:116)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:291)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:53)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:167)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:60)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-10185c47e326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o811.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 189.0 failed 1 times, most recent failure: Lost task 2.0 in stage 189.0 (TID 637, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:116)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:291)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:53)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:167)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:60)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "print(\"RunTime %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-65-0d26aebfcbc7>\", line 7, in <module>\n",
      "    cvmodel = crossval.fit(vectorformat3)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\", line 227, in _fit\n",
      "    df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\", line 469, in rand\n",
      "    jc = sc._jvm.functions.rand(seed)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37312)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/os.py\", line 722, in __getitem__\n",
      "    value = self._data[self.encodekey(key)]\n",
      "KeyError: b'COLUMNS'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:37312)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-0d26aebfcbc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                           \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                           numFolds=3)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcvmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorformat3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mrandCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_rand\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mrand\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:37312)"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5]).build()\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "cvmodel = crossval.fit(vectorformat3)\n",
    "result = cvmodel.bestModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`data` should be an RDD of LabeledPoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f0e94f5690f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictionAndLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RunTime %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, lambda_)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`data` should be an RDD of LabeledPoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainNaiveBayesModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNaiveBayesModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `data` should be an RDD of LabeledPoint"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
