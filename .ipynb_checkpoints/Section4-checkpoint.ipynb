{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BDAS').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correct data schema.\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "               StructField('year',IntegerType(),True),\n",
    "               StructField('month',IntegerType(),True),\n",
    "               StructField('day',IntegerType(),True),\n",
    "               StructField('hour',IntegerType(),True),\n",
    "               StructField('PM25',FloatType(),True),\n",
    "               StructField('SO2',FloatType(),True),\n",
    "               StructField('NO2',FloatType(),True),\n",
    "               StructField('CO',FloatType(),True),\n",
    "               StructField('O3',FloatType(),True),\n",
    "               StructField('TEMP',FloatType(),True),\n",
    "               StructField('PRES',FloatType(),True),\n",
    "               StructField('DewPointTempeature',FloatType(),True),\n",
    "               StructField('RAIN',FloatType(),True),\n",
    "               StructField('WindDirection',StringType(),True),\n",
    "               StructField('WindSpend',FloatType(),True),\n",
    "               StructField('station',StringType(),True),\n",
    "               StructField('NQR',StringType(),True),\n",
    "               StructField('Season',StringType(),True),\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"../final_dataset/final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+---+---+---+---+----+----+------------------+----+-------------+---------+-------+---+------+\n",
      "|year|month|day|hour|PM25|SO2|NO2| CO| O3|TEMP|PRES|DewPointTempeature|RAIN|WindDirection|WindSpend|station|NQR|Season|\n",
      "+----+-----+---+----+----+---+---+---+---+----+----+------------------+----+-------------+---------+-------+---+------+\n",
      "+----+-----+---+----+----+---+---+---+---+----+----+------------------+----+-------------+---------+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.NQR == 'Very Poor').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol ='WindDirection', outputCol= 'WindDirectionIndex')\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['WindDirection','station','NQR','Season']\n",
    "for i in column:\n",
    "    indexer = StringIndexer(inputCol =i, outputCol= i+\"Index\" )\n",
    "    model = indexer.fit(df)\n",
    "    indexed = model.transform(df)\n",
    "    df = indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(*column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----+----+----+-----+----+----+------+------------------+----+---------+------------------+------------+--------+-----------+\n",
      "|year|month|day|hour|PM25| SO2| NO2|   CO|  O3|TEMP|  PRES|DewPointTempeature|RAIN|WindSpend|WindDirectionIndex|stationIndex|NQRIndex|SeasonIndex|\n",
      "+----+-----+---+----+----+----+----+-----+----+----+------+------------------+----+---------+------------------+------------+--------+-----------+\n",
      "|2015|    1|  1|   2| 3.0|10.0|15.0|400.0|55.0| 0.0|1027.0|             -22.9| 0.0|      2.4|               1.0|         7.0|     0.0|        0.0|\n",
      "|2015|    1|  1|   3| 4.0|13.0|13.0|400.0|57.0| 0.0|1028.0|             -24.4| 0.0|      2.4|               1.0|         7.0|     0.0|        0.0|\n",
      "|2015|    1|  1|   4| 4.0|15.0|12.0|400.0|58.0| 0.0|1030.0|             -24.4| 0.0|      2.4|               1.0|         7.0|     0.0|        0.0|\n",
      "|2015|    1|  1|   5| 3.0|13.0|15.0|500.0|55.0|-1.0|1024.0|             -24.4| 0.0|      3.2|              11.0|         7.0|     0.0|        0.0|\n",
      "|2015|    1|  1|   6| 3.0|10.0|30.0|500.0|39.0|-4.0|1029.0|             -24.3| 0.0|      0.9|              15.0|         7.0|     0.0|        0.0|\n",
      "+----+-----+---+----+----+----+----+-----+----+----+------+------------------+----+---------+------------------+------------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "formula = RFormula(\n",
    "    formula=\"NQRIndex ~ .\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "\n",
    "output = formula.fit(new_df).transform(new_df)\n",
    "feature_selection = output.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 4 features selected\n",
      "+--------------------+-----+--------------------+\n",
      "|            features|label|    selectedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|[2015.0,1.0,1.0,2...|  0.0|[2015.0,1.0,1.0,2.0]|\n",
      "|[2015.0,1.0,1.0,3...|  0.0|[2015.0,1.0,1.0,3.0]|\n",
      "|[2015.0,1.0,1.0,4...|  0.0|[2015.0,1.0,1.0,4.0]|\n",
      "|[2015.0,1.0,1.0,5...|  0.0|[2015.0,1.0,1.0,5.0]|\n",
      "|[2015.0,1.0,1.0,6...|  0.0|[2015.0,1.0,1.0,6.0]|\n",
      "|[2015.0,1.0,1.0,8...|  0.0|[2015.0,1.0,1.0,8.0]|\n",
      "|[2015.0,1.0,1.0,9...|  0.0|[2015.0,1.0,1.0,9.0]|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,1.0,1...|  0.0|[2015.0,1.0,1.0,1...|\n",
      "|[2015.0,1.0,2.0,4...|  0.0|[2015.0,1.0,2.0,4.0]|\n",
      "|[2015.0,1.0,2.0,5...|  0.0|[2015.0,1.0,2.0,5.0]|\n",
      "|[2015.0,1.0,2.0,6...|  0.0|[2015.0,1.0,2.0,6.0]|\n",
      "|[2015.0,1.0,2.0,7...|  0.0|[2015.0,1.0,2.0,7.0]|\n",
      "|[2015.0,1.0,2.0,8...|  0.0|[2015.0,1.0,2.0,8.0]|\n",
      "|[2015.0,1.0,2.0,9...|  0.0|[2015.0,1.0,2.0,9.0]|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,2.0,1...|  0.0|[2015.0,1.0,2.0,1...|\n",
      "|[2015.0,1.0,5.0,1...|  0.0|[2015.0,1.0,5.0,1...|\n",
      "|[2015.0,1.0,5.0,1...|  0.0|[2015.0,1.0,5.0,1...|\n",
      "|[2015.0,1.0,5.0,1...|  0.0|[2015.0,1.0,5.0,1...|\n",
      "|[2015.0,1.0,5.0,1...|  0.0|[2015.0,1.0,5.0,1...|\n",
      "|[2015.0,1.0,5.0,2...|  0.0|[2015.0,1.0,5.0,2...|\n",
      "|[2015.0,1.0,5.0,2...|  0.0|[2015.0,1.0,5.0,2...|\n",
      "|[2015.0,1.0,5.0,2...|  0.0|[2015.0,1.0,5.0,2...|\n",
      "|[2015.0,1.0,6.0,0...|  0.0|[2015.0,1.0,6.0,0.0]|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1.0]|\n",
      "|[2015.0,1.0,6.0,2...|  0.0|[2015.0,1.0,6.0,2.0]|\n",
      "|[2015.0,1.0,6.0,3...|  0.0|[2015.0,1.0,6.0,3.0]|\n",
      "|[2015.0,1.0,6.0,4...|  0.0|[2015.0,1.0,6.0,4.0]|\n",
      "|[2015.0,1.0,6.0,6...|  0.0|[2015.0,1.0,6.0,6.0]|\n",
      "|[2015.0,1.0,6.0,7...|  0.0|[2015.0,1.0,6.0,7.0]|\n",
      "|[2015.0,1.0,6.0,8...|  0.0|[2015.0,1.0,6.0,8.0]|\n",
      "|[2015.0,1.0,6.0,9...|  0.0|[2015.0,1.0,6.0,9.0]|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  0.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,6.0,1...|  1.0|[2015.0,1.0,6.0,1...|\n",
      "|[2015.0,1.0,7.0,5...|  0.0|[2015.0,1.0,7.0,5.0]|\n",
      "|[2015.0,1.0,7.0,6...|  0.0|[2015.0,1.0,7.0,6.0]|\n",
      "|[2015.0,1.0,7.0,7...|  0.0|[2015.0,1.0,7.0,7.0]|\n",
      "|[2015.0,1.0,7.0,8...|  0.0|[2015.0,1.0,7.0,8.0]|\n",
      "|[2015.0,1.0,7.0,9...|  0.0|[2015.0,1.0,7.0,9.0]|\n",
      "|[2015.0,1.0,7.0,1...|  0.0|[2015.0,1.0,7.0,1...|\n",
      "|[2015.0,1.0,8.0,1...|  1.0|[2015.0,1.0,8.0,1.0]|\n",
      "|[2015.0,1.0,8.0,2...|  1.0|[2015.0,1.0,8.0,2.0]|\n",
      "|[2015.0,1.0,8.0,3...|  1.0|[2015.0,1.0,8.0,3.0]|\n",
      "|[2015.0,1.0,8.0,4...|  2.0|[2015.0,1.0,8.0,4.0]|\n",
      "|[2015.0,1.0,8.0,5...|  2.0|[2015.0,1.0,8.0,5.0]|\n",
      "|[2015.0,1.0,9.0,6...|  0.0|[2015.0,1.0,9.0,6.0]|\n",
      "|[2015.0,1.0,9.0,7...|  0.0|[2015.0,1.0,9.0,7.0]|\n",
      "|[2015.0,1.0,9.0,8...|  0.0|[2015.0,1.0,9.0,8.0]|\n",
      "|[2015.0,1.0,9.0,9...|  0.0|[2015.0,1.0,9.0,9.0]|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,9.0,1...|  0.0|[2015.0,1.0,9.0,1...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,11.0,...|  0.0|[2015.0,1.0,11.0,...|\n",
      "|[2015.0,1.0,12.0,...|  2.0|[2015.0,1.0,12.0,...|\n",
      "|[2015.0,1.0,12.0,...|  1.0|[2015.0,1.0,12.0,...|\n",
      "|[2015.0,1.0,12.0,...|  1.0|[2015.0,1.0,12.0,...|\n",
      "|[2015.0,1.0,12.0,...|  1.0|[2015.0,1.0,12.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "|[2015.0,1.0,16.0,...|  0.0|[2015.0,1.0,16.0,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=4, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "result = selector.fit(feature_selection).transform(feature_selection)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.select('features','label','selectedFeatures').show(100,truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-54386f830356>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-54386f830356>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    val data = Array(\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n",
    "  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    ")\n",
    "val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(3)\n",
    "  .fit(df)\n",
    "\n",
    "val result = pca.transform(df).select(\"pcaFeatures\")\n",
    "result.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=int(2), maxDepth=int(4), labelCol='label', seed=11)\n",
    "model = rf.fit(feature_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SO2': 0.0008204226356877305, 'TEMP': 0.0, 'WindDirectionIndex': 0.001688752584229212, 'year': 0.0, 'WindSpend': 0.019777215140169512, 'PRES': 0.0, 'CO': 0.10418290298996702, 'DewPointTempeature': 0.006172533666245003, 'NQRIndex': 0.0, 'NO2': 0.0015246934803152445, 'O3': 0.0, 'month': 0.0, 'hour': 0.0002349291764291249, 'stationIndex': 0.0, 'RAIN': 0.0, 'day': 0.0, 'PM25': 0.8655985503269572}\n"
     ]
    }
   ],
   "source": [
    "ff=model.featureImportances\n",
    "importancesList=[float(col) for col in  ff]\n",
    "colList = new_df.columns\n",
    "result=dict(zip(colList,importancesList))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85796 36692 20584 723 0 0 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "df_0 = feature_selection.filter(col(\"label\") == 0)\n",
    "df_1 = feature_selection.filter(col(\"label\") == 1)\n",
    "df_2 = feature_selection.filter(col(\"label\") == 2)\n",
    "df_3 = feature_selection.filter(col(\"label\") == 3)\n",
    "df_4 = feature_selection.filter(col(\"label\") == 4)\n",
    "df_5 = feature_selection.filter(col(\"label\") == 5)\n",
    "df_6 = feature_selection.filter(col(\"label\") == 6)\n",
    "print(df_0.count(),df_1.count(),df_2.count(),df_3.count(),df_4.count(),df_5.count(),df_6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"clicked\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o241.fit.\n: java.lang.IllegalArgumentException: Field \"clicked\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\tat org.apache.spark.ml.feature.ChiSqSelector.transformSchema(ChiSqSelector.scala:183)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:159)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:123)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-47bd2c98bec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                          outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ChiSqSelector output with top %d features selected\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumTopFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"clicked\" does not exist.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2015.0,1.0,1.0,2.0,3.0,10.0,15.0,400.0,55.0,0.0,1027.0,-22.899999618530273,0.0,2.4000000953674316,1.0,7.0,0.0]), 0.0,),\n",
    "    (Vectors.dense([2015.0,1.0,1.0,3.0,4.0,13.0,13.0,400.0,57.0,0.0,1028.0,-24.399999618530273,0.0,2.4000000953674316,1.0,7.0,0.0]), 0.0,),\n",
    "    (Vectors.dense([2015.0,1.0,6.0,18.0,60.0,28.0,74.0,1100.0,20.0,0.0,1016.0,-18.700000762939453,0.0,0.8999999761581421,13.0,7.0,0.0]), 1.0,)], \n",
    "    [ \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(feature_selection).transform(feature_selection)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionname(value):\n",
    "    return Vectors.dense(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "udfsomefunc = F.udf(functionname, VectorUDT())\n",
    "featurestest = feature_selection.withColumn(\"testfeature\", udfsomefunc(\"features\"))\n",
    "featurestest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=4, featuresCol=\"testfeature\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "result = selector.fit(featurestest).transform(featurestest)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealWithFeatures =  VectorAssembler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = new_df.columns\n",
    "data_columns = [x for x in data_columns if x not in ['day','PM25','NQRIndex']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dealWithFeatures.(Vectors.dense(setInputCols(data_columns))).setOutputCol(\"features\").transform(new_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_columns = new_df.columns\n",
    "new_data_columns =  [x for x in data_columns if x not in ['features','NQRIndex']]\n",
    "print(new_data_columns)\n",
    "data = data.drop(*new_data_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "selector = ChiSqSelector(numTopFeatures=16, featuresCol=\"features\",outputCol=\"selectedFeatures\", labelCol=\"NQRIndex\")\n",
    "result = selector.fit(data).transform(data)\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show(30, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"Select\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=2, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"Select\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
